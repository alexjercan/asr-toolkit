{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_AudioBooksToText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSiqSDo0oIL1"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "!apt-get install libsox-fmt-all libsox-dev sox > /dev/null\n",
        "!pip install torchaudio\n",
        "!python -m pip install git+https://github.com/facebookresearch/WavAugment.git > /dev/null\n",
        "!pip install wandb\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "# install beam search decoder\n",
        "!apt-get install -y swig\n",
        "!git clone https://github.com/NVIDIA/NeMo -b \"$BRANCH\"\n",
        "!cd NeMo && bash scripts/asr_language_modeling/ngram_lm/install_beamsearch_decoders.sh\n",
        "\n",
        "%rm -rf asr\n",
        "!git clone https://github.com/alexjercan/asr-toolkit.git asr > /dev/null\n",
        "\n",
        "\"\"\"\n",
        "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
        "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
        "that you want to use the \"Run All Cells\" (or similar) option.\n",
        "\"\"\"\n",
        "# exit()\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch1MxTVao8KZ",
        "outputId": "f630c402-f786-440b-d474-0486681a7763"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "import nemo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import augment\n",
        "import torchaudio\n",
        "import torchaudio.datasets\n",
        "from torchaudio.datasets.librispeech import load_librispeech_item\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "from datetime import datetime as dt\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from asr.metrics import ASRMetricFunction, CTCLossFunction\n",
        "from asr.visualisation import play_audio, print_err_html, print_stats, plot_waveform\n",
        "from asr.general import set_parameter_requires_grad, load_checkpoint, save_checkpoint, tensors_to_device, tensor_to_string\n",
        "from asr.models import BeamSearchDecoderWithLM\n",
        "from asr.datasets import LibriSpeechBookDataset\n",
        "from IPython.display import YouTubeVideo, clear_output\n",
        "clear_output()\n",
        "\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME='stt_en_jasper10x5dr'\n",
        "LM_3GRAM_PATH = '3-gram.arpa'\n",
        "ROOT = os.path.join(\".\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mOLd6dlpQZk"
      },
      "source": [
        "class LibriSpeechBookDataset(torchaudio.datasets.LIBRISPEECH):\n",
        "    def __init__(self, root, url, folder_in_archive=\"LibriSpeech\", download=False):\n",
        "        super(LibriSpeechBookDataset, self).__init__(root, url, folder_in_archive, download)\n",
        "\n",
        "        chapterpaths = {p.stem:str(p) for p in Path(self._path).glob('*/*/')}\n",
        "        names = [\"ID\", \"READER\", \"MINUTES\", \"SUBSET\", \"PROJ.\", \"BOOK ID\", \"CH. TITLE\", \"PROJECT TITLE\"]\n",
        "        converters = {\"BOOK ID\": str.strip, \"SUBSET\": str.strip, \"CH. TITLE\" : str.strip, \"PROJECT TITLE\" : str.strip}\n",
        "        chapters = os.path.join(root, folder_in_archive, \"CHAPTERS.TXT\")\n",
        "        df = pd.read_csv(chapters, delimiter='|', comment=';', names=names, converters=converters)\n",
        "        df = df[df[\"SUBSET\"] == os.path.basename(url)].groupby(\"BOOK ID\")\n",
        "        df = pd.DataFrame({\"CHAPTERS\": df[\"ID\"].apply(list), \"MINUTES\": df[\"MINUTES\"].apply(sum)})\n",
        "        df['CHAPTER_PATH'] = df.apply(lambda row: [chapterpaths[str(x)] for x in row[\"CHAPTERS\"]], axis=1)\n",
        "        df.reset_index(level=0, inplace=True)\n",
        "        df = df.astype({\"BOOK ID\": 'object'})\n",
        "\n",
        "        names = [\"BOOK ID\", \"BOOK TITLE\"]\n",
        "        converters = {\"BOOK ID\": str.strip, \"BOOK TITLE\": str.strip}\n",
        "        books = os.path.join(root, folder_in_archive, \"BOOKS.TXT\")\n",
        "        dfp = pd.read_csv(books, delimiter='|', comment=';', names=names, converters=converters, usecols=names)\n",
        "        df = pd.merge(df, dfp, how=\"inner\", on=\"BOOK ID\")\n",
        "\n",
        "        self._walker = df\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        row = self._walker.iloc[n]\n",
        "\n",
        "        audiofileids = [str(p.stem) for chapterpath in row[\"CHAPTER_PATH\"] for p in Path(chapterpath).glob('*' + self._ext_audio)]\n",
        "        items = [load_librispeech_item(fileid, self._path, self._ext_audio, self._ext_txt) for fileid in audiofileids]\n",
        "\n",
        "        waveforms, _, utterances, _, _, _ = zip(*items)\n",
        "        return torch.cat(waveforms, dim=1), \" \".join(utterances), row[\"BOOK TITLE\"], row[\"MINUTES\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEc5H3bFpLAy"
      },
      "source": [
        "def download_lm(lm_path):\n",
        "    %rm -v \"{lm_path}\"*\n",
        "    !wget \"https://www.openslr.org/resources/11/{lm_path}.gz\" -O \"{lm_path}.gz\"\n",
        "    !gzip -cdv \"{lm_path}.gz\" > \"{lm_path}\"\n",
        "\n",
        "model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=MODEL_NAME, strict=False).to(DEVICE)\n",
        "\n",
        "VOCABULARY = list(map(lambda x: x.upper(), model.decoder.vocabulary))\n",
        "vocab = VOCABULARY + ['<pad>']\n",
        "BLANK = len(vocab) - 1 \n",
        "\n",
        "DICTIONARY = dict(zip(vocab, range(len(vocab))))\n",
        "LABELS = {v:k for k, v in DICTIONARY.items()}\n",
        "\n",
        "train_dataset = LibriSpeechBookDataset(root=ROOT, url=\"train-clean-100\", folder_in_archive=\"LibriSpeech\", download=True)\n",
        "dev_dataset = LibriSpeechBookDataset(root=ROOT, url=\"dev-clean\", folder_in_archive=\"LibriSpeech\", download=True)\n",
        "test_dataset = LibriSpeechBookDataset(root=ROOT, url=\"test-clean\", folder_in_archive=\"LibriSpeech\", download=True)\n",
        "\n",
        "if not os.path.exists(LM_3GRAM_PATH):\n",
        "    download_lm(LM_3GRAM_PATH)\n",
        "beam_search_lm = BeamSearchDecoderWithLM(\n",
        "    vocab=VOCABULARY,\n",
        "    beam_width=16,\n",
        "    alpha=1.5, beta=1.5,\n",
        "    lm_path=LM_3GRAM_PATH,\n",
        "    num_cpus=max(os.cpu_count(), 1))\n",
        "clear_output()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_Zv8lQ1huTV"
      },
      "source": [
        "def get_best_transcriptions(transcriptions):\n",
        "    return list(map(lambda xs: xs[0][1], transcriptions))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "Ws9lxkwhkjug",
        "outputId": "7be50f4a-e151-4a06-9c1e-39fa7e3061af"
      },
      "source": [
        "loop = tqdm(train_dataset, position=0, leave=True)\n",
        "df = pd.DataFrame(None, columns=[\"REAL TEXT\", \"BOOK TITLE\", \"DURATION\"])\n",
        "\n",
        "for batch_idx, (waveform, transcription, booktitle, duration) in enumerate(loop):\n",
        "    df = df.append({\"REAL TEXT\": transcription, \"BOOK TITLE\": booktitle, \"DURATION\": duration}, ignore_index=True)\n",
        "\n",
        "loop.close()\n",
        "df.to_csv(\"train-clean-100.csv\")\n",
        "files.download(\"train-clean-100.csv\")\n",
        "\n",
        "print(df[\"REAL TEXT\"].apply(lambda t: len(t.split(\" \"))).describe())\n",
        "print(df[\"DURATION\"].describe())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 305/305 [03:13<00:00,  1.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ccf79db2-9229-49a1-9a0c-2a36538a3a5c\", \"train-clean-100.csv\", 5279692)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "count      305.000000\n",
            "mean      3226.773770\n",
            "std       2456.083601\n",
            "min         78.000000\n",
            "25%       1445.000000\n",
            "50%       2792.000000\n",
            "75%       4135.000000\n",
            "max      19479.000000\n",
            "Name: REAL TEXT, dtype: float64\n",
            "count    305.000000\n",
            "mean      19.675639\n",
            "std       15.336142\n",
            "min        0.540000\n",
            "25%        9.170000\n",
            "50%       17.020000\n",
            "75%       25.110000\n",
            "max      127.620000\n",
            "Name: DURATION, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H64SHq54hq5r"
      },
      "source": [
        "model.eval()\n",
        "loop = tqdm(train_dataset, position=0, leave=True)\n",
        "df = pd.DataFrame(None, columns=[\"TEXT\", \"REAL TEXT\", \"BOOK TITLE\", \"DURATION\"])\n",
        "\n",
        "for batch_idx, (waveform, transcription, booktitle, duration) in enumerate(loop):\n",
        "    waveform = waveform[0].to(DEVICE).unsqueeze(0)\n",
        "    valid_lengths = torch.tensor([waveform.shape[-1]], device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    best_transcriptions = get_best_transcriptions(transcriptions)\n",
        "    df = df.append({\"TEXT\": best_transcriptions[0], \"REAL TEXT\": transcription, \"BOOK TITLE\": booktitle, \"DURATION\": duration}, ignore_index=True)\n",
        "\n",
        "loop.close()\n",
        "df.to_csv(\"train-clean-100.csv\")\n",
        "files.download(\"train-clean-100.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQPJ9waaTPHw",
        "outputId": "373e5c67-1e67-4b2c-a3c0-adec246e4e3c"
      },
      "source": [
        "model.eval()\n",
        "loop = tqdm(test_dataset, position=0, leave=True)\n",
        "df = pd.DataFrame(None, columns=[\"TEXT\", \"REAL TEXT\", \"BOOK TITLE\", \"DURATION\"])\n",
        "\n",
        "for batch_idx, (waveform, transcription, booktitle, duration) in enumerate(loop):\n",
        "    waveform = waveform[0].to(DEVICE).unsqueeze(0)\n",
        "    valid_lengths = torch.tensor([waveform.shape[-1]], device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    best_transcriptions = get_best_transcriptions(transcriptions)\n",
        "    df = df.append({\"TEXT\": best_transcriptions[0], \"REAL TEXT\": transcription, \"BOOK TITLE\": booktitle, \"DURATION\": duration}, ignore_index=True)\n",
        "\n",
        "loop.close()\n",
        "df.to_csv(\"test-clean.csv\")\n",
        "files.download(\"test-clean.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/54 [00:00<?, ?it/s][NeMo W 2021-08-16 11:09:03 patch_utils:50] torch.stft() signature has been updated for PyTorch 1.7+\n",
            "    Please update PyTorch to remain compatible with later versions of NeMo.\n",
            "[NeMo W 2021-08-16 11:09:03 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "      return torch.floor_divide(self, other)\n",
            "    \n",
            " 35%|███▌      | 19/54 [04:47<08:19, 14.28s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK05xl_Phfc-"
      },
      "source": [
        "model.eval()\n",
        "loop = tqdm(dev_dataset, position=0, leave=True)\n",
        "df = pd.DataFrame(None, columns=[\"TEXT\", \"REAL TEXT\", \"BOOK TITLE\", \"DURATION\"])\n",
        "\n",
        "for batch_idx, (waveform, transcription, booktitle, duration) in enumerate(loop):\n",
        "    waveform = waveform[0].to(DEVICE).unsqueeze(0)\n",
        "    valid_lengths = torch.tensor([waveform.shape[-1]], device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs, encoded_len, greedy_predictions = model(input_signal=waveform, input_signal_length=valid_lengths)\n",
        "        transcriptions = beam_search_lm(log_probs=log_probs, log_probs_length=encoded_len)\n",
        "\n",
        "    best_transcriptions = get_best_transcriptions(transcriptions)\n",
        "    df = df.append({\"TEXT\": best_transcriptions[0], \"REAL TEXT\": transcription, \"BOOK TITLE\": booktitle, \"DURATION\": duration}, ignore_index=True)\n",
        "\n",
        "loop.close()\n",
        "df.to_csv(\"dev-clean.csv\")\n",
        "files.download(\"dev-clean.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX3TCRdXo7jP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}